# awesome-dLLM-resources
Frequently updated list of dLLM (Diffusion Large Language Models) papers, models, and other resources.

Maintained by Suhas Pai and Xiaojun Ren

## Models
- **Large Language Diffusion Models** — *February 14* <i><a href="https://arxiv.org/abs/2502.09992" target="_blank">arXiv</a></i>
- **LLaDA 1.5: Variance-Reduced Preference Optimization for Large Language Diffusion Models** — *May 25* <i><a href="https://arxiv.org/abs/2505.19223" target="_blank">arXiv</a></i>
- **LLaDA-MoE: A Sparse MoE Diffusion Language Model** - *September 25* <i><a href="https://arxiv.org/abs/2509.24389" target="_blank">arXiv</a></i>
- **UltraLLaDA: Scaling the Context Length to 128K for Diffusion Large Language Models** — *October 12* <i><a href="https://arxiv.org/abs/2510.10481" target="_blank">arXiv</a></i>
- **Soft-Masked Diffusion Language Models** - *October 20* <i><a href="https://arxiv.org/abs/2510.17206" target ="_blank">arXiv</a></i>
- **DiffuCoder: Understanding and Improving Masked Diffusion Models for Code Generation** - *June 25* <i><a href="https://arxiv.org/abs/2506.20639" target ="_blank">arXiv</a></i>

## Reasoning in dLLMs
- **Improving Reasoning for Diffusion Language Models via Group Diffusion Policy Optimization** — *October 9* <i><a href="https://arxiv.org/abs/2510.08554" target="_blank">arXiv</a></i>
- **DiFFPO: Training Diffusion LLMs to Reason Fast and Furious via Reinforcement Learning** — *October 2* <i><a href="https://arxiv.org/abs/2510.02212" target="_blank">arXiv</a></i>
- **RFG: Test-Time Scaling for Diffusion Large Language Model Reasoning with Reward-Free Guidance** — *September 29* <i><a href="https://arxiv.org/abs/2509.25604" target="_blank">arXiv</a></i>
- **d1: Scaling Reasoning in Diffusion Large Language Models via Reinforcement Learning** — *June 3* <i><a href="https://arxiv.org/abs/2504.12216" target="_blank">arXiv</a></i>
- **Enhancing Reasoning for Diffusion LLMs via Distribution Matching Policy Optimization** — *October 9* <i><a href="https://arxiv.org/abs/2510.08233" target="_blank">arXiv</a></i>


## Scaling laws
- **Diffusion Language Models are Super Data Learners** — *October 1* <i><a href="https://jinjieni.github.io/dlms-are-super-data-learners/resources/pdf/Diffusion_Language_Models_are_Super_Data_Learners.pdf" target="_blank">Github</a></i>
- **Diffusion Beats Autoregressive in Data-Constrained Settings** — *July 21* <i><a href="https://arxiv.org/abs/2507.15857" target="_blank">arXiv</a></i>

## Architectures
- **Syntax-Guided Diffusion Language Models with User-Integrated Personalization** — *October 2* <i><a href="https://arxiv.org/abs/2510.01028" target="_blank">arXiv</a></i>
- **Revolutionizing reinforcement learning framework for diffusion large language models** — *September 8* <i><a href="https://arxiv.org/abs/2509.06949" target="_blank">arXiv</a></i>
- **CoDA: Coding LM via Diffusion Adaptation** — *September 27* <i><a href="https://arxiv.org/abs/2510.03270" target="_blank">arXiv</a></i>
- **Fast-dLLM v2: Efficient Block-Diffusion LLM** — *September 30* <i><a href="https://arxiv.org/abs/2509.26328" target="_blank">arXiv</a></i>
- **Diffusion LLM with Native Variable Generation Lengths: Let [EOS] Lead the Way** — *October 28* <i><a href="https://arxiv.org/abs/2510.24605" target="_blank">arXiv</a></i>


## Finetuning/RL
- **WeFT: Weighted Entropy-driven Fine-Tuning for dLLMs** - *September 25* <i><a href="https://arxiv.org/abs/2509.20863" target="_ blank">arXiv</a></i
- **Inpainting-Guided Policy Optimization for Diffusion Large Language Models** - *September 12* <i><a href="https://arxiv.org/abs/2509.10396" target="_ blank">arXiv</a></i>
- **Principled and Tractable RL for Reasoning with Diffusion Language Models** - *October 5* <i><a href="https://arxiv.org/abs/2510.04019" target="_ blank">arXiv</a></i>

## Decoding Strategies
- **Unveiling the Potential of Diffusion Large Language Model in Controllable Generation** - *July 6* <i><a href="https://arxiv.org/abs/2507.04504" target="_ blank">arXiv</a></i>
- **Time Is a Feature: Exploiting Temporal Dynamics in Diffusion Language Models** - *August 12* <i><a href="https://arxiv.org/abs/2508.09138" target="_ blank">arXiv</a></i>
- **AdaBlock-dLLM: Semantic-Aware Diffusion LLM Inference via Adaptive Block Size** - *September 30* <i><a href="https://arxiv.org/abs/2509.26432" target="_ blank">arXiv</a></i>
- **Self Speculative Decoding for Diffusion Large Language Models** - *October 5* <i><a href="https://arxiv.org/abs/2510.04147" target="_ blank">arXiv</a></i>
- **CreditDecoding: Accelerating Parallel Decoding in Diffusion Large Language Models with Trace Credits** - *October 7* <i><a href="https://arxiv.org/abs/2510.06133" target="_ blank">arXiv</a></i>
- **Beyond Static Cutoffs: One-Shot Dynamic Thresholding for Diffusion Language Models** - *November 3* <i><a href="https://arxiv.org/abs/2511.02077" target="_ blank">arXiv</a></i>






## Inference
- **Spiffy: Multiplying Diffusion LLM Acceleration via Lossless Speculative Decoding** — *September 22* <i><a href="https://arxiv.org/abs/2509.18085" target="_blank">arXiv</a></i>
- **Thinking Inside the Mask: In-Place Prompting in Diffusion LLMs** — *August 14* <i><a href="https://arxiv.org/abs/2508.10736" target="_blank">arXiv</a></i>
- **Dllmquant: Quantizing diffusion-based large language models** — *August 14* <i><a href="https://arxiv.org/abs/2508.14090" target="_blank">arXiv</a></i>
- **Sparse-dLLM: Accelerating Diffusion LLMs with Dynamic Cache Eviction** — *August 4* <i><a href="https://arxiv.org/abs/2508.02558" target="_blank">arXiv</a></i>
- **Accelerating Diffusion Large Language Models with SlowFast Sampling: The Three Golden Principles** — *June 12* <i><a href="https://arxiv.org/abs/2506.10848" target="_blank">arXiv</a></i>
- **Accelerating Diffusion LLMs via Adaptive Parallel Decoding** — *May 31* <i><a href="https://arxiv.org/abs/2506.00413" target="_blank">arXiv</a></i>
- **dLLM-Cache: Accelerating Diffusion Large Language Models with Adaptive Caching** — *May 17* <i><a href="https://arxiv.org/abs/2506.06295" target="_blank">arXiv</a></i>
- **Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV Cache and Parallel Decoding** — *May 28* <i><a href="https://arxiv.org/abs/2505.22618" target="_blank">arXiv</a></i>
- **Quantization Meets dLLMs: A Systematic Study of Post-training Quantization for Diffusion LLMs** — *August 20* <i><a href="https://arxiv.org/abs/2508.14896" target="_blank">arXiv</a></i>
- **Quant-dLLM: Post-Training Extreme Low-Bit Quantization for Diffusion Large Language Models** — *September 27* <i><a href="https://arxiv.org/abs/2510.03274" target="_blank">arXiv</a></i>
- **d^2Cache: Accelerating Diffusion-Based LLMs via Dual Adaptive Caching** — *September 27* <i><a href="https://arxiv.org/abs/2509.23094" target="_blank">arXiv</a></i>
- **Ultra-Fast Language Generation via Discrete Diffusion Divergence Instruct** — *September 29* <i><a href="https://arxiv.org/abs/2509.25035" target="_blank">arXiv</a></i>
- **Free Draft-and-Verification: Toward Lossless Parallel Decoding for Diffusion Large Language Models** — *September 30* <i><a href="https://arxiv.org/abs/2510.00294" target="_blank">arXiv</a></i>
- **Accelerating Diffusion LLM Inference via Local Determinism Propagation** — *October 8* <i><a href="https://arxiv.org/abs/2510.07081" target="_blank">arXiv</a></i>
- **dInfer: An Efficient Inference Framework for Diffusion Language Models** — *October 9* <i><a href="https://arxiv.org/abs/2510.08666" target="_blank">arXiv</a></i>
- **Mask Tokens as Prophet: Fine-Grained Cache Eviction for Efficient dLLM Inference** — *October 10* <i><a href="https://arxiv.org/abs/2510.09309" target="_blank">arXiv</a></i>











## Security
- **Jailbreaking Large Language Diffusion Models: Revealing Hidden Safety Flaws in Diffusion-Based Text Generation** — *July 25* <i><a href="https://arxiv.org/abs/2507.19227" target="_blank">arXiv</a></i>
- **The Devil behind the mask: An emergent safety vulnerability of Diffusion LLMs** — *July 15* <i><a href="https://arxiv.org/abs/2507.11097" target="_blank">arXiv</a></i>
- **Where to Start Alignment? Diffusion Large Language Model May Demand a Distinct Position** — *August 17* <i><a href="https://arxiv.org/abs/2508.12398" target="_blank">arXiv</a></i>
- **DiffuGuard: How Intrinsic Safety is Lost and Found in Diffusion Large Language Models** — *September 29* <i><a href="https://arxiv.org/abs/2509.24296" target="_blank">arXiv</a></i>



## Applications
- **Discovering Mathematical Equations with Diffusion Language Model** — *September 16* <i><a href="https://arxiv.org/abs/2509.13136" target="_blank">arXiv</a></i>
- **Harnessing LLM for Noise-Robust Cognitive Diagnosis in Web-Based Intelligent Education Systems** — *October 5* <i><a href="https://arxiv.org/abs/2510.04093" target="_blank">arXiv</a></i>


## Survey
- **A Survey on Diffusion Language Models** — *August 14* <i><a href="https://arxiv.org/abs/2508.10875" target="_blank">arXiv</a></i>
- **Diffusion-based Large Language Models Survey** — *August 26* <i><a href="https://www.techrxiv.org/users/952417/articles/1321784-diffusion-based-large-language-models-survey" target="_blank">TechRxiv</a></i>
- **Discrete Diffusion in Large Language and Multimodal Models: A Survey** — *June 16* <i><a href="https://arxiv.org/abs/2506.13759" target="_blank">arXiv</a></i>

